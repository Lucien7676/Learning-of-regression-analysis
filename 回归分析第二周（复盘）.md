# 简单线性回归

## 最小二乘法估计的性质

$$y = \beta_0 + \beta_1x + \epsilon \ ,\quad E(\epsilon|x) = 0\ , \quad Var(\epsilon|x) = \sigma^2$$

假设$(x_i,y_i)$是来自$(x,y)$的随机样本，则模型也可以表示为：

$$y_i = \beta_0 + \beta_1 x_i+\epsilon_i\ ,E(\epsilon_i) = 0 \ , \quad Var(\epsilon_i) = \sigma^2$$

上面介绍==最小化残差平方和==$\alpha = \sum^n_{i=1}(y_i - \hat\beta_0 - \hat\beta_1x)^2$可得==OLS估计==为

$$\hat\beta_1 = \frac{\sum_{i=1}^ny_ix_i - n\bar x \bar y}{\sum_{i=1}^nx_i^2 - n \bar x^2} = \frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sum_{i=1}^n(x_i - \bar x)^2} = \frac{\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\frac{1}{n-1}\sum_{i=1}^n(x_i - \bar x)^2} = \frac{S_{xy}}{S_{xx}}$$

$$\hat\beta_0 = \bar y - \hat \beta_1 \bar x$$

下面我们将通过==求偏导数得到最小化平方和==的参数
$$
\left\{
\begin{array}{rcl}
\left.\frac{\partial{\alpha}}{\partial\hat\beta_0} \right| _{\hat\beta_0,\hat\beta_1} = 0\\
\left.\frac{\partial{\alpha}}{\partial\hat\beta_1} \right| _{\hat\beta_0,\hat\beta_1} = 0
\end{array} \right.

\Longrightarrow


\left\{
\begin{array}{rcl}
\left.\sum_{i=1}^n(y_i - \hat\beta_0-\hat\beta_1x_i)\right|_{\hat\beta_0,\hat\beta_1} = 0\\
\left.\sum_{i=1}^n(y_i - \hat\beta_0-\hat\beta_1x_i)(x_i)\right|_{\hat\beta_0,\hat\beta_1} = 0
\end{array} \right.


\Longrightarrow


\left\{
\begin{array}{rcl}
\sum_{i=1}^n e_i = 0 \\
\sum_{i=1}^n e_ix_i = 0
\end{array} \right.
\quad (*)
$$

- 性质：
  - 1⃣️ $\sum_{i=1}^n e_i = 0$
  - 2⃣️ $(\bar x, \bar y)$在样本回归线上
  - 3⃣️ ${e_i}$和${x_i}$样本相关系数为0，${e_i}$和${x_i}$不相关
    - $\sum e_ix_i = \sum (e_i - \bar e)x_i = \sum (e_i-\bar e)(x_i - \bar x) = \sum (e_i - \bar e)x_i -\bar x\sum(e_i-\bar e) = \sum(e_i - \bar e)x_i = 0$
  - 4⃣️ $e_i$和$\hat y_i$不相关
    - $\sum e_i \hat y_i = \sum e_i(\hat \beta_0 + \hat \beta_1 x_i) = \hat \beta_0 \sum e_i + \hat \beta_1 \sum e_i x_i = 0 = \sum(e_i - \bar e)(y_i - \bar y)$

下面我们将会研究==OLS估计量的统计性质==，具体地，我们将会证明

-  OLS估计为==线性估计==，即$\hat \beta_1$可以表示为$y_1,\cdots,y_n$线性组合形式
  $$
  \begin{eqnarray*}
  \hat \beta_1 & = & \frac{S_{xy}}{S_xx} \\
  & = & \frac{\sum(s_i-\bar x)(y_i - \bar y)}{\sum (x_i-\bar x)^2} \\
  & = & \sum \frac{x_i - \bar x}{S_{xx}}y_i - \sum \frac{x_i - \bar x}{S_{xx}} \bar y \\
  & = & \sum \frac{x_i - \bar x}{S_{xx}}y_i \\
  & = & \sum c_i y_i \\
  & = & \sum c_i(\beta_0 + \beta_1x_i + \epsilon_i)\\
  & = & \beta_0 \sum c_i + \beta_1 \sum c_ix_i + \sum c_i\epsilon_i \\
  & = & \beta_1 + \sum c_i \epsilon_i
  \end{eqnarray*}
  $$

  $$
  c_i = \frac{x_i - \bar x}{S_{xx}}\\
  E(\hat \beta_1) = \beta_1 + \sum c_i E(\epsilon_i) = \beta_1 \\
  bias(\hat \beta_1) = E(\hat \beta_1) - \beta_1 = 0 \quad \hat \beta_1 为\beta_1的线性无偏估计\\
  同理，我们可以得到\\
  \begin{eqnarray*}
  \hat \beta_0 & = & \bar y - \hat \beta_1 \bar x \\
  & = & \frac{1}{n} \sum y_i - \bar x \sum c_iy_i\\
  & = & \sum [\frac{1}{n} - \bar x c_i]y_i \\
  & = & \sum d_iy_i \\
  & = & \beta_0 \sum d_i + \beta_1 \sum d_ix_i + \sum d_i \epsilon_i\\
  & = & \beta_0 + \sum d_i \epsilon_i
  \end{eqnarray*}
  \\
  E(\hat \beta_0) = E(\beta_0) = \beta_0
  $$

  

  
  $$
  \begin{eqnarray*}
  Var(\hat \beta_1) & = & Var(\beta_1 + \sum c_i \epsilon_i)\\
  & = & Var(\sum c_i \epsilon_i)\\
  & = & \sum c_i^2 \sigma^2\\
  & = & \sigma^2 \sum_{i=1}^n\frac{(x_i - \bar x)^2}{S^2_{xx}}\\
  & = & \frac{\sigma^2}{S_{xx}}
  \end{eqnarray*}
  $$

  $$
  \begin{eqnarray*}
  Var(\hat \beta_0) & = & \sum d_i^2\sigma^2 \\
  & = & \sum (\frac{1}{n}-\bar x \frac{x_i - \bar x}{S_{xx}})^2 \sigma^2\\
  & = & (\frac{1}{n} + \frac{\bar x^2}{S_{xx}})\sigma^2
  \end{eqnarray*}
  $$

  书上的证明方法
  $$
  \begin{eqnarray*}
  Var(\hat \beta_0) & = & Var(\bar y - \bar x \hat \beta_1)\\
  & = & Var(\bar y) + Var(\bar x \hat \beta_1) - 2Cov(\bar y, \bar x \hat \beta_1) \\
  & = & \frac{1}{n^2} \sum Var(\epsilon_i) + \bar x^2 Var(\hat \beta_1) \\
  & = & (\frac{1}{n} + \frac{\bar x^2}{S_{xx}})\sigma^2
  
  \end{eqnarray*}
  $$

  ## 随机误差方差$\sigma^2$ 的估计

  ==SLR==$(simple \ linear \ regression)$
  $$
  y_i = \beta_0 + \beta_1 x + \epsilon_i \ , \quad \epsilon_i  ～  (0,\sigma^2)
  $$
  通过上面的计算，我们可以得到以下的参数估计的均方误差
  $$
  MSE(\hat \beta_1) = Var(\hat \beta_1) = \frac{\sigma^2}{S_{xx}}\\
  MSE(\hat \beta_0) = Var(\hat \beta_0) = (\frac{1}{n}+\frac{\bar x^2}{S_{xx}})\sigma^2
  $$
  从上面的式子中，我们可以发现，在估计==参数的均方误差==的过程，实际上我们只需要进行==估计$\sigma$==即可，接下来，我们将会对==估计$\sigma$展开工作==。
  $$
  首先估计\sigma^2有以下的式子，但是在本例子中，我们只有样本数据，\\因此我们需要通过样本数据残差e_i来对\epsilon_i进行一个估计\\
  \begin{eqnarray*}
  \hat \sigma^2 & = & \frac{1}{n-2} \sum e_i^2\\
  & = & \frac{1}{n-2}\sum (y_i-\hat\beta_0 - \hat\beta_1x_i)^2
  \end{eqnarray*}
  \quad 此处n-2表示的是自由度，在(*)中有两个限制条件\\
  $$
  
  $$
  \begin{align}
  \Longrightarrow &
  E(\hat \sigma^2) = \sigma^2\\
  
  \Longrightarrow &
  E(SS_{res}) = E(\sum e_i^2) = (n-2)E(\hat\sigma^2) = (n-2)\sigma^2\\
  
  \Longrightarrow &
  \frac{SS_{res}}{n-2} 为\sigma^2的无偏估计\\
  
  \Longrightarrow &
  \hat\sigma^2 = \frac{SS_{res}}{n-2} = MS_{res}
  \end{align}
  $$
  
  
  下面引入一个概念==回归标准误差==
  $$
  s.e.(\hat\beta_1) = \frac{\hat\sigma}{\sqrt{S_{xx}}} = \sqrt{\frac{SS_{res}}{S_{xx}(n-2)}}\\
  s.e.(\hat\beta_0) = \sqrt{\frac{1}{n} + \frac{\bar x^2}{S_{xx}}}\hat\sigma = \sqrt{(\frac{1}{n} + \frac{\bar x^2}{S_{xx}})\frac{SS_{res}}{n-2}}
  $$
  
  ## 火箭推进器数据代码
  
  ```R
  # 首先先进行读取数据
  da2_1 = readxl::read_xls("./DataSets4e/Chapter 2/Examples/data-ex-2-1 (Rocket Prop).xls")
  colnames(da2_1) <- c("ID","y","x")
  da2_1
  # 下面使用fit函数对其进行拟合
  fit <- lm(y~x,data = da2_1)
  res <- resid(fit) # 抽取残差值序列
  SS_res <- sum(res^2)
  # 下面直接进行输出残差平方和
  SS_res
  
  
  # 下面我们先进行估计对应的sigma_hat
  # 首先进行读取维度的信息
  n <- dim(da2_1)[1]
  df <- n-2
  sigma_hat <- sqrt(SS_res/df)
  # 下面进行输出sigma_hat
  sigma_hat
  
  
  # 接下来使用对sigma的参数估计对参数的标准误差进行估计
  x = da2_1$x
  Sxx = sum((x-mean(x))^2)
  s.e.beta1 <- sigma_hat/sqrt(Sxx)
  s.e.beta0 <- sigma_hat*sqrt((1/n + mean(x)^2/Sxx))
  s.e.beta1
  s.e.beta0
  
  # 使用函数的形式对拟合的数据进行输出
  summary(fit)
  ```
  
  ## 简单线性模型的另外一种表达形式
  
  对于简单线性模型的原本的形式：
  $$
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i
  $$
  对于给定的数据，我们能够将上面的表达式进行变形
  $$
  \begin{eqnarray*}
  y_i & = & \beta_0 + \beta_1(x_i - \bar x) + \beta_1 \bar x + \epsilon_i\\
  & = & (\beta_0 + \beta_1 \bar x) + \beta_1(x_i - \bar x) + \epsilon_i\\
  & = & \beta_0’ + \beta_1(x_i - \bar x) + \epsilon_i \\
  & = & \beta_0' + \beta_1x_i' + \epsilon_i
  \end{eqnarray*}
  $$
  将初始的形式转换为下面的标准的形式的时候，对于==斜率的参数估计不会改变==，但是==对于截距的估计是会改变的==
  $$
  标准形式下的\beta_0和\beta_1的参数估计和原始形式下的\beta_0和\beta_1\\
  \hat \beta_1 = \frac{S_{x'y}}{S_{x'x'}} = \frac{\sum(x'-\bar x')(y - \bar y)}{\sum(x' - \bar x')^2}\quad \hat \beta_0' = \bar y - \hat \beta_1 \bar x' = \bar y \\
  \hat \beta_1 = \frac{S_{xy}}{S_{xx}} = \frac{\sum(x-\bar x)(y - \bar y)}{\sum(x-\bar x)^2} \quad \hat \beta_0 = \bar y - \hat \beta_1 \bar x
  $$
  
  - 将简单的线性模型进行标准化的特点如下
    - 此时==截距的估计和斜率的估计是不相关的==，即$Cov(\hat \beta_0',\hat \beta_1) = 0$，对于第二个式子，我们可以发现
    - 如果我们用该模型进行预测，则有$\hat y = \bar y + \hat \beta_1 \cdot (x - \bar x)$，==提醒我们回归模型的有效范围在其均值点附近==
  
  
  
  
  
  